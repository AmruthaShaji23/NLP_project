{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d21c5492",
   "metadata": {},
   "source": [
    "# Code to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e657244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2b5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CHOICE = \"TinyLlama\"  # Changing this to test different models\n",
    "\n",
    "# Available models:\n",
    "MODELS = {\n",
    "    \"Dolly-v2-12B\": \"databricks/dolly-v2-12b\",\n",
    "    \"Phi-3\": \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    \"Qwen-7B\": \"Qwen/Qwen2.5-7B-Instruct\",\n",
    "    \"Gemma-2B\": \"google/gemma-2b-it\",\n",
    "    \"TinyLlama\": \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "}\n",
    "\n",
    "MODEL_NAME = MODELS[MODEL_CHOICE]\n",
    "\n",
    "print(f\"\\n Testing on {MODEL_CHOICE} ({MODEL_NAME})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e030ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see if there is GPU\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"No GPU!\")\n",
    "    exit()\n",
    "else:\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\\\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c74b576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files\n",
    "FILES = {\n",
    "    'Hindi': 'Examples for annotations - Hindi.csv',\n",
    "    'Dutch': 'Examples for annotations - Dutch.csv',\n",
    "    'Romanian': 'Examples for annotations - Romanian.csv'\n",
    "}\n",
    "\n",
    "EMOTIONS = ['Joy', 'Sadness', 'Anger', 'Fear', 'Disgust', 'Surprise', 'NULL']\n",
    "\n",
    "# Loadind the model\n",
    "\n",
    "try:\n",
    "    # 4-bit quantization to save memory\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\"\n",
    "    )\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Load model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.float16,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "    print(f\"✅ {MODEL_CHOICE} loaded successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {str(e)[:200]}\")\n",
    "    if \"token\" in str(e).lower() or \"gated\" in str(e).lower():\n",
    "        print(\"\\nNeed to go to HuggingFace to load the key\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850a8e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to test the prediction\n",
    "def predict_emotion(text, language):\n",
    "\n",
    "    # Create simple prompt\n",
    "    prompt = f\"\"\"Analyze this text and identify the PRIMARY emotion.\n",
    "\n",
    "Text: \"{text[:300]}\"\n",
    "Language: {language}\n",
    "\n",
    "Choose ONE emotion: Joy, Sadness, Anger, Fear, Disgust, Surprise, or NULL\n",
    "\n",
    "Emotion:\"\"\"\n",
    "\n",
    "    try:\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=600)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=10,\n",
    "                temperature=0.1,\n",
    "                do_sample=False,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "\n",
    "        # Extract emotion\n",
    "        for emotion in EMOTIONS:\n",
    "            if emotion.lower() in response.lower():\n",
    "                return emotion\n",
    "\n",
    "        # Return first word if no match\n",
    "        return response.strip().split()[0] if response.strip() else \"NULL\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)[:50]}\")\n",
    "        return \"ERROR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ef9570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on all 3 languages\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for lang, filepath in FILES.items():\n",
    "    print(f\"\\n Testing {lang}...\")\n",
    "\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        continue\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(filepath)\n",
    "    emotion_col = 'Annotated Emotions' if 'Annotated Emotions' in df.columns else 'Annotated Emotions 1'\n",
    "\n",
    "    # Get examples with emotions\n",
    "    df_test = df[df[emotion_col].notna() & (df[emotion_col] != 'NULL')]\n",
    "\n",
    "    if len(df_test) == 0:\n",
    "        print(f\"No examples found\")\n",
    "        continue\n",
    "\n",
    "    # Test each example\n",
    "    correct = 0\n",
    "    results = []\n",
    "\n",
    "    for idx, row in df_test.iterrows():\n",
    "        text = str(row['Text'])\n",
    "        true_emotion = str(row[emotion_col])\n",
    "\n",
    "        print(f\"   Testing {len(results)+1}/10...\", end='\\r')\n",
    "\n",
    "        predicted = predict_emotion(text, lang)\n",
    "\n",
    "        is_correct = (predicted == true_emotion)\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "\n",
    "        results.append({\n",
    "            'text': text[:60],\n",
    "            'true': true_emotion,\n",
    "            'predicted': predicted,\n",
    "            'correct': is_correct\n",
    "        })\n",
    "\n",
    "    accuracy = (correct / len(results) * 100) if results else 0\n",
    "\n",
    "    print(f\"\\n   ✅ {lang}: {accuracy:.1f}% ({correct}/10)\")\n",
    "\n",
    "    # Show a few examples\n",
    "    print(f\"Sample predictions:\")\n",
    "    for i, r in enumerate(results[:3], 1):\n",
    "        status = \"✅\" if r['correct'] else \"❌\"\n",
    "        print(f\"      {status} True: {r['true']:10s} | Pred: {r['predicted']:10s}\")\n",
    "\n",
    "    all_results.append({\n",
    "        'model': MODEL_CHOICE,\n",
    "        'language': lang,\n",
    "        'accuracy': accuracy,\n",
    "        'correct': correct,\n",
    "        'total': len(results),\n",
    "        'results': results\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee4c0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "\n",
    "if all_results:\n",
    "    # Save JSON\n",
    "    output_file = f\"{MODEL_CHOICE.replace('.', '_')}_results.json\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    # Create summary\n",
    "    summary_df = pd.DataFrame([{\n",
    "        'Model': r['model'],\n",
    "        'Language': r['language'],\n",
    "        'Accuracy': f\"{r['accuracy']:.1f}%\",\n",
    "        'Correct': r['correct'],\n",
    "        'Total': r['total']\n",
    "    } for r in all_results])\n",
    "\n",
    "    csv_file = f\"{MODEL_CHOICE.replace('.', '_')}_summary.csv\"\n",
    "    summary_df.to_csv(csv_file, index=False)\n",
    "\n",
    "    print(f\"Saved: {output_file}\")\n",
    "    print(f\"Saved: {csv_file}\")\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n SUMMARY:\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "\n",
    "    overall_correct = sum(r['correct'] for r in all_results)\n",
    "    overall_total = sum(r['total'] for r in all_results)\n",
    "    overall_accuracy = (overall_correct / overall_total * 100) if overall_total > 0 else 0\n",
    "\n",
    "    print(f\"\\n OVERALL: {overall_accuracy:.1f}% ({overall_correct}/{overall_total})\")\n",
    "\n",
    "    # Download files\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        print(\"\\n Downloading results...\")\n",
    "        files.download(csv_file)\n",
    "        files.download(output_file)\n",
    "        print(\" Downloaded!\")\n",
    "    except:\n",
    "        print(\"\\n Download manually from file browser ←\")\n",
    "\n",
    "else:\n",
    "    print(\"No results to save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ae5e98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
